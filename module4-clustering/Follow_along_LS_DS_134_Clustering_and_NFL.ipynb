{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Follow_along_LS_DS_134_Clustering_and_NFL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndow33/DS-Unit-1-Sprint-3-Linear-Algebra/blob/master/module4-clustering/Follow_along_LS_DS_134_Clustering_and_NFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y9bK8maEkopO"
      },
      "source": [
        "# Part 0. Principle Components Analysis Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4b1ThzZUhP",
        "colab_type": "text"
      },
      "source": [
        "### 0.1 Notes about PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3JHDhIiPks0z"
      },
      "source": [
        "**Principal Components are not a re-labeling of the original features**\n",
        "\n",
        "I saw some confusion yesterday about what the new Principal Components are that come out of our PCA transformations. Principal Components are a linear combination of any and all dimensions (features) that will increase their variance, this means that PCs are made up of a mixture of features --mostly the ones with the highest variance, but also smaller parts from other features. This means that they are not comparable to the original features of our $X$ matrix. In cases where we're not reducing dimensionality that much (like the Iris dataset) our Principal Components might be extremely similar to the original features (since there's not that many to pull from) but don't think of them in that way, think of them as a completely new dataset that we can't really apply "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8CPPjEHVktro"
      },
      "source": [
        "**PCA does not make predictions**\n",
        "\n",
        "I would not call PCA a \"machine learning algorithm\" in that it does not try to make any predictions. We can't calculate any accuracy measure. You can call it an algorithm, you can call it a preprocessing technique or method, but it's not truly making predictions. This may have been confusing due to the fact that the Iris dataset had labels, but PCA is just re-organizing points in space, it's not making any predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FK6eHmtDktur"
      },
      "source": [
        "**PCA does not standardize the data for you**\n",
        "\n",
        "You'll notice in the \"from scratch\" implementation of PCA that I did in class yesterday that in that example I did not divide the points by the standard deviation. I believe you'll get a slightly different set of points if you choose to divide by the standard deviation (I think this might be what A Apte was seeing yesterday when he tried both methods and found that they looked different. It could be something else entirely, but that's my first guess at what could be going on.)\n",
        "\n",
        "The Sklearn implementation does not standardize the points for you as part of the process. You can either do this yourself \"by hand\" or you can use other sklearn methods like this preprocessing step which will automatically standardize your data to have a mean of 0 and a standard deviation of 1. You have to do this **before** you pass your data to PCA.\n",
        "\n",
        "<https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nwfQC5Yxksx1"
      },
      "source": [
        "**PCA does not retain 100% of the information of the original dataset**\n",
        "\n",
        "Each component explains a certain % of the variance of the original dataset. PCA tries to maximize that variance, but you might need to use more than 2 components. \n",
        "\n",
        "Typically you want to use enough components in your analysis to keep the explained variance > 90%.\n",
        "\n",
        "So we're trading off losing a small-medium amount of predictive power for a reduction in dimensions/size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xi9ks-kq_j8J"
      },
      "source": [
        "## 0.2 Housing Affordability Data System (HADS) Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIriwYycZUhZ",
        "colab_type": "text"
      },
      "source": [
        "The Housing Affordability Data System (HADS) is a set of files derived from the 1985 and later national American Housing Survey (AHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3igUQDzZUha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read Natinal Data \n",
        "national_url = 'https://www.huduser.gov/portal/datasets/hads/hads2013n_ASCII.zip'\n",
        "national_file = 'thads2013n.txt'\n",
        "\n",
        "if os.path.exists(national_file):\n",
        "    national = pd.read_csv(national_file)\n",
        "else: \n",
        "    z_national = urlopen(national_url)\n",
        "    zip_national = ZipFile(BytesIO(z_national.read())).extract(national_file)\n",
        "    national = pd.read_csv(zip_national)\n",
        "\n",
        "national.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lvm0d_ZZUhf",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess all the categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d9rfKtulRX7J",
        "colab": {}
      },
      "source": [
        "# make lists of categorical and numeric columns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5tm3lUmZUhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Whar are the cat columns?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqS52tD-ZUhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how many do we have of each?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjTAGU4NRXbd",
        "colab": {}
      },
      "source": [
        "# Cast all the categorical columns to \"category\" data type\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YwyFCpMbRXD9",
        "colab": {}
      },
      "source": [
        "# Remove apostrophes from all of the categorical columns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUzBGchHZUhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many numeric columns do we now have?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm0ufP6YZUh2",
        "colab_type": "text"
      },
      "source": [
        "### 0.2.2 Principal Components Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BTSjlNgKRWtW",
        "colab": {}
      },
      "source": [
        "# Make a copy of our dataframe, we will standarize the copy so as to not overwrite our original data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_BgMhe9A9ZC",
        "colab": {}
      },
      "source": [
        "# instantiate the SKLearn class for standardization\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coSy4LRXZUh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the dataset (default is normalization)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m0Y5qJAs-1Q6",
        "colab": {}
      },
      "source": [
        "# import and instantiate the PCA class\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpkTMiEoZUiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply PCA to the data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc9i8xvpZUiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  how much variation did each principal component explain?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z355cgSVZUiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How much total variance did we explain?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upre1NIsZUiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How much information did we lose?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QODFhs6ZUiO",
        "colab_type": "text"
      },
      "source": [
        "### 0.2.3 Make a scree plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoWcNqhhZUiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the number of components\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKjWzFCnZUiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a numpy array as long as the number of components\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5djdHocZUiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an array of 10 values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-72lx5zZUiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the variance explained by each component.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dehTE3eNZUia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the cumulative variance explained by all the components.\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CmxL457j-Q2y",
        "colab": {}
      },
      "source": [
        "# Define scree plot function\n",
        "def scree_plot():\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    ax = plt.subplot(111)\n",
        "   \n",
        "    ax.xaxis.set_tick_params(width=0)\n",
        "    ax.yaxis.set_tick_params(width=2, length=12)\n",
        "\n",
        "    ax.set_xlabel(\"Principal Component\")\n",
        "    ax.set_ylabel(\"Variance Explained (%)\")\n",
        "    plt.title('Explained Variance Per Principal Component')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "adiMr78LGpE-",
        "colab": {}
      },
      "source": [
        "# Apply the plot function to our principle component\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzE5btZ9ZUii",
        "colab_type": "text"
      },
      "source": [
        "# Part 1. Intro to Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TBXSEvwbODsA"
      },
      "source": [
        "### 2.1 Machine Learning Overview\n",
        "\n",
        "How do you know what kind of Machine Learning that you're doing? What algorithm should you pick? \n",
        "\n",
        "This decision is driven by:\n",
        "\n",
        "1) The attributes of your dataset\n",
        "\n",
        "2) What you want to predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t4qW9d7FlHUy"
      },
      "source": [
        "__Supervised Learning__  \n",
        "\n",
        "  - Classification algorithms try to predict the correct category (or class) from a given set of categories.\n",
        "  - Regression algorithms predict a continuous or semi-continuous value. (Not to be confused with _Linear_ Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NynJ4Ye9lHg0"
      },
      "source": [
        "__Unsupervised Learning__\n",
        "  - Clustering\n",
        "  Identifying groupings of related observations. This is our topic for today!\n",
        "  - Dimensionality Reduction\n",
        "  Takes a high-dimensionality dataset and reduces the number of variables taken into consideration via methods of feature selection and feature extraction.\n",
        "  - Association Rule Learning\n",
        "  Association is a method of discovering relationships between observations in a dataset. (between ovservations or features, not just relationships between explanatory variables and a single output variable. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ARo_wctGlHpT"
      },
      "source": [
        "__Reinforcement Learning__  \n",
        "* A form of machine learning where an \"agent\" interacts with its environment and is rewarded for correct behavior and penalized for incorrect behavior. \n",
        "* Over many iterations the agent learns the behavior that results in the greatest reward and smallest punishment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2nPOjACaubCv"
      },
      "source": [
        "Memorize This!\n",
        "\n",
        "**Supervised**: Labelled outputs\n",
        "- **Classification**: Discrete output cagetories\n",
        "- **Regression**: Continuous output values\n",
        "\n",
        "**Unsupervised**: Outputs are not labelled\n",
        "\n",
        "**Reinforcement**: Rewards/punishments for \"behaviors\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K9YeIVBQoAJR"
      },
      "source": [
        "Kaggle datasets:\n",
        "training data has labels but the testing data does not.\n",
        "\n",
        "[Classification Examples](https://github.com/ShuaiW/kaggle-classification)\n",
        "\n",
        "[Regression Examples](https://github.com/ShuaiW/kaggle-classification)\n",
        "\n",
        "[Unsupervised Learning Examples](http://www.lsi.upc.edu/~bejar/apren/docum/trans/09-clusterej-eng.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yzdWvN9iugsd"
      },
      "source": [
        "ML Cheat Sheets  \n",
        "<div>\n",
        "<img https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet-small_v_0_6-01.png width='200' />\n",
        "</div>\n",
        "\n",
        "\n",
        "![Microsoft Cheat Sheet](https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet-small_v_0_6-01.png)\n",
        "\n",
        "This one does not group them by supervised, unsupervised, regression, classification, etc. But it gives you an idea of the different families of algorithms.\n",
        "\n",
        "![Algorithm Map](https://jixta.files.wordpress.com/2015/11/machinelearningalgorithms.png?w=816&h=521&zoom=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QEEQapU2uRIx"
      },
      "source": [
        "### 1.2 No Free Lunch Principle\n",
        "\n",
        "The no free lunch principle states that the more an algorithm is optimized to solve one specific kind of problem, the worse it gets at solving all other kinds of problems. \n",
        "\n",
        "This means that if you want an algorithm that's really good at solving a certain problem (cluster shape for example), it usually lose some of its ability to generalize to other problems. \n",
        "\n",
        "**What does this mean for us as data scientists?**\n",
        "\n",
        "1) There are always tradeoffs when selecting from different approaches. Because of this, understanding those tradeoffs and justifying your choice of methodology is just as important as actually doing the work itself.\n",
        "\n",
        "2) The only way that we can choose one approach over another is to make assumptions about our data. If we don't know anything about the characteristics of our data, then we can't make an informed choice of algorithm. \n",
        "\n",
        "Think about how we knew to use Unsupervised vs Supervised learning for the clustering problem, the choice was informed by our data. Does it have labels or not? \n",
        "\n",
        "![No Free Lunch](https://cdn-images-1.medium.com/max/1600/1*oNt9G9UpVhtyFLDBwEMf8Q.png)\n",
        "\n",
        "Density Based Clustering Animation:\n",
        "\n",
        "[DB Scan Animation](https://www.youtube.com/watch?v=h53WMIImUuc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sZLcehHCL6DM",
        "colab": {}
      },
      "source": [
        "Tips  \n",
        "* Don't Get Overwhelmed! Some people spend their entire careers researching new clustering methods and improvements.\n",
        "\n",
        "* Don't be a perfectionist! There are too many techniques to master, you can't learn all of them in 9 months.\n",
        "\n",
        "* Focus on learning within the context of a problem you want to solve or a project that you are passionate about building"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GjCxoJFNl0ly"
      },
      "source": [
        "### 1.3 Clustering \n",
        "\n",
        "Clustering falls into the category of unsupervised learning. This is because there is nothing in our training data that designates the correct cluster that a data point should belong to beforehand. In fact, there's not even a \"correct\" _**number**_ of clusters to assign our points to. We will discuss some heuristics for choosing an **appropriate** number of clusters, but this (as in much of data science) is an area where there is no cut and dry right and wrong answer. \n",
        "\n",
        "Remember: \"All models are wrong, but some models are useful.\" Data science is all about acknowledging where your model might be wrong while still pursuing something useful. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c6S6TOtgl0sb"
      },
      "source": [
        "**Why Clustering?**\n",
        "\n",
        "Clustering answers questions about how similar or dissimilar our \"data objects\" are. Clustering is one of the most effective methods for summarizing datasets with this question in mind. Clustering can be thought of as a sort of \"unsupervised classification.\" You will likely never deploy a clustering model to a production environment, they're too unreliable. Clustering is more useful as a tool for data exploration than a model for making predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yDspwZ9Tl0yG"
      },
      "source": [
        "“Clustering isn’t hard—it’s either easy, or not interesting”\n",
        "\n",
        "If a good clustering exists, then it usually can be efficiently found. Clustering is the most difficult when clear clusters don't exist in the first place. In that case you should question whether or not clustering is the most appropriate or useful method. \n",
        "\n",
        "The purpose of clustering is to group data points that are similar along certain specified dimensions (attributes). \"Similarity\" is defined as the points being close together in some n-dimensional space. \n",
        "\n",
        "The greater the number of dimensions, the more difficult clustering becomes because the increase in dimensions makes all points this is because measures of distance are used to determine similarity between datapoints, and the greater the dimensionality the more all points become roughly equidistant with one another. (We don't have time to go further into this or demonstrate this, but clustering suffers from performance and interpretability issues in a high number of dimensions). Some of these challenges can be rectified by choosing an appropriate measure of \"distance\" between data points. For example, using clustering for document analysis is still fairly effective even though the analysis is of a highly-dimenaional space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOxEiYGUZUix",
        "colab_type": "text"
      },
      "source": [
        "**Applications of Clustering**\n",
        "\n",
        "Astronomy: There's too much data from space for us to look at each individual start and galaxy and categorize it, but we can cluster them intro groups based on their observable attributes. \n",
        "\n",
        "[SkyCat](http://www.eso.org/sci/observing/tools/skycat.html)\n",
        "\n",
        "[Sloan Digital Sky Survey](https://www.sdss.org/)\n",
        "\n",
        "Document Classification / Grouping - We'll need to study a little bit of NLP before we can get into this. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ldJ0c24yl02e"
      },
      "source": [
        "### 1.4 Types of Clustering\n",
        "\n",
        "**Hierarchical:**\n",
        "\n",
        "    - Agglomerative: start with individual points and combine them into larger and larger clusters\n",
        "    - Divisive: Start with one cluster and divide the points into smaller clusters.\n",
        "\n",
        "**Point Assignment:**\n",
        "\n",
        "    - We decide on a number of clusters out of the gate, and assign points to that number of clusters.\n",
        "\n",
        "**Hard vs Soft Clustering**\n",
        "    - Hard Clustering assigns a point to a cluster\n",
        "    - Soft Clustering assigns each point a probability that it's in a given cluster.\n",
        "    - We're going to only deal with hard clustering, it's the more traditional approach. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mnAcqxeDl06U"
      },
      "source": [
        "### 1.5 Clustering Distance Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WhmN7D0Ol0-E"
      },
      "source": [
        "**Distance Measures**\n",
        "\n",
        "Did you know that there are distance measures other than euclidean distance?\n",
        "\n",
        "- Euclidean\n",
        "- Cosine\n",
        "- Jaccard\n",
        "- Edit Distance\n",
        "- Etc. \n",
        "\n",
        "Clustering traditionally uses Euclidean Distance, but this particular measure of distance breaks down in high dimensionality spaces. It's what we'll use for today. If you **LOVE**  clustering and want to put a strong focus on this area of Machine learning (at the expense of focusing strongly on others) then I would suggest further personal research into different clustering algorithms and distance measures. \n",
        "\n",
        "I want to reiterate that you don't have to use PCA and clustering in conjunction with each other. I think it's more common that they are not used together, but it can be useful in certain cases. We might try it today for fun and so reiterate how PCA is the preprocessing step, and K-means will be the main \"Machine Learning Algorithm.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CY95oSIT-5ko"
      },
      "source": [
        "There are a lot of clustering algorithms. \n",
        "\n",
        "YOU DON'T NEED TO BE ABLE TO CODE ALL OF THEM FROM SCRATCH IN ORDER TO APPLY THEM OR EVEN TO UNDERSTAND THEM. FOCUS ON LEARNING THINGS WITHIN THE CONTEXT OF A PROBLEM YOU ARE TRYING TO SOLVE AND ONLY LEARN THOSE THINGS THAT WILL HELP YOU SOLVE THE PROBLEM. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rJx_PxNUmyDT"
      },
      "source": [
        "# Part 2. K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGSeEtls_QXU"
      },
      "source": [
        "### 2.1 The Process\n",
        "\n",
        "Given a set of points in n-dimensional space we want to :\n",
        "\n",
        "1) select k random points to act as initial centroids (one point for each cluster)\n",
        "\n",
        "2) Find the cluster of points surrounding that centroid (assign points to the centroid that they lie closest to)\n",
        "\n",
        "3) Calculate a new centroid for the cluster\n",
        "\n",
        "Repeat steps 2 & 3 until the model converges. (Clusters don't change)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lt03ADtDByNX",
        "colab": {}
      },
      "source": [
        "# import the blob maker\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc_H34zWZUi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Let's make some blobs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auiDsjLYZUi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make that into a dataframe of x, y and label values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH5nlMIbZUi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the clusters we made\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x3fV8Cw_Eq6w"
      },
      "source": [
        "**Linear Separability**\n",
        "The 2D blobs below are what is called \"linearly separable\" Meaning that we could use straight lines to separate them with no errors. This is the most trivial case of of k-means clustering, but it will help us to demonstrate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jK624RjW-qGk",
        "colab": {}
      },
      "source": [
        "# Drop labels to prove that this is truly unsupervised learning\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ro4aO_mPoP1v",
        "colab": {}
      },
      "source": [
        "# Scatter plot of our label-less data (no more colors!)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WzETDJC2FNDK"
      },
      "source": [
        "### 2.2. K-means clustering by hand\n",
        "\n",
        "K-means clustering is what's known as a centroid-based clustering algorithm. A centroid is an imaginary point located at the average location of all of the points in a given cluster. For example, if I wanted to find the centroid of all of the points in the above graph I would just calculate the average of the dataset's x-coordinates to find the x value of the centroid, and the average of the dataset's y-coordinates to find the y value of the centroid.\n",
        "\n",
        "If we plot the centroid on the graph you'll see that it lies in the middle of the points. You could imagine the centroid as if it is the center of gravity, or center of mass for a given cluster. Since in this example we're treating all of the points in the dataset as if they're in the same cluster, it will end up somewhere in the middle. We're just doing this to demonstrate what a centroid is. The K-means algorithm doesn't ever calculate the centroid for the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63sXiekPZUjG",
        "colab_type": "text"
      },
      "source": [
        "**Re-review steps of the algorithm**\n",
        "\n",
        "Given a set of points in n-dimensional space we want to:\n",
        "\n",
        "1) select k random points to act as initial centroids (one point for each cluster)\n",
        "2) Find the cluster of points surrounding that centroid (assign points to the centroid that they lie closest to)\n",
        "3) Calculate a new centroid for the cluster\n",
        "\n",
        "Repeat steps 2 & 3 until the model converges. (Clusters don't change)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVJBRn_9ZUjG",
        "colab_type": "text"
      },
      "source": [
        "**3-means clustering**\n",
        "\n",
        "Lets pick k=3 and start demonstrating how this algorithm actually works. \n",
        "\n",
        "The k-means algorithm works by picking 3 of the actual datapoints at random (in the simplest case) and treating those as the starting centroids. Using those centroids, 3 clusters are calculated.\n",
        "\n",
        "We then use the new clusters and calculate a new centroid for each of them. Then, using those centroids we re-cluster. We perform this process over and over again until our clusters stabilize and the centroids stop moving. Lets demonstrate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xLZyrzOE_CnM",
        "colab": {}
      },
      "source": [
        "# Calculate the centroid of the entire dataset (only for demonstration purposes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVjJ4gj7ZUjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# what are the x and y coords of that centroid?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8OgBWw0c_cVq",
        "colab": {}
      },
      "source": [
        "# Display the plots and their centroid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OvuDTEOM_4Ch",
        "colab": {}
      },
      "source": [
        "# Sample random points to serve as the initial fake \"centroids\". These will get updated.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eXz0-YRuAGIQ",
        "colab": {}
      },
      "source": [
        "# Plot initial \"fake\" centroids on the graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTBDDraYZUjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's import the big guns.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nbwx8_pA3DU",
        "colab": {}
      },
      "source": [
        "# Calculate the Nearest Centroid to each data point\n",
        "def find_nearest_centroid():\n",
        "    \n",
        "    # calculate the distances between each point and each centroid\n",
        "    \n",
        "\n",
        "    # Get nearest centroid to each point based on distance\n",
        "    \n",
        "    # convert to a Pandas series and return values\n",
        "\n",
        "    # return entire dataframe\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbr1ZR-iZUjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check out our 'points' dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im5elrSxZUje",
        "colab_type": "text"
      },
      "source": [
        "**first pass**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ACmU-SzWoJ6-",
        "colab": {}
      },
      "source": [
        "# Take a first pass at calculating the nearest centroid to each point\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcRyfL70ZUjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# have the centroids moved at all?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XVZQLvfAFh60",
        "colab": {}
      },
      "source": [
        "# Define a function to plot clusters\n",
        "def plot_clusters(df, column_header, centroids):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I_6cOfkeFwWO",
        "colab": {}
      },
      "source": [
        "# Define a function to get centroids\n",
        "def get_centroids(df, column_header):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrnBYQGXZUjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the function. Have the centroids changed at all?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgKDjYaoZUjt",
        "colab_type": "text"
      },
      "source": [
        "**second pass**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qyZLdP_nHKBa",
        "colab": {}
      },
      "source": [
        "# Get Clusters for New Centroids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg4lF4EiZUjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot New Cluster\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itDIznREZUjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the function. Have the centroids changed at all?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g5Xvx0SZUjy",
        "colab_type": "text"
      },
      "source": [
        "**third pass**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBueXP01G8tE",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "\n",
        "# Plot New Cluster\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOL0tQxiZUj0",
        "colab_type": "text"
      },
      "source": [
        "**fourth pass**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8u6KiGdvpD-e",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "\n",
        "# Plot New Cluster\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUmuuYeCZUj3",
        "colab_type": "text"
      },
      "source": [
        "**fifth pass**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jaTbGpAcpPSf",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "\n",
        "# Plot New Cluster\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK5IQ_qVZUj4",
        "colab_type": "text"
      },
      "source": [
        "**sixth pass**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gXpje6GWpbYS",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "\n",
        "# Plot New Cluster\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ8bzWomZUj8",
        "colab_type": "text"
      },
      "source": [
        "**convergence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKo9xSypJvPd",
        "colab": {}
      },
      "source": [
        "# When additional passes fail to create any change, we have hit \"convergence\".\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1-nJ_i-NQZIM"
      },
      "source": [
        "How many centroids == number of means (that's the K in k-means clustering). Since the centroid is the mean of a cluster, the number of centroids to choose is the most important decision to make in \"k-means\" clustering. The K value is the number of centroids.\n",
        "\n",
        "Congratulations, you've just been introduced to the first method of _**picking k**_ - Just graph your points and pick a number that makes sense. This gets a lot harder once you get a dimensionality higher than 3, but... Didn't we learn about some way to take high dimensional data and turn it into 2 or 3 dimensions...?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TGueCso5SFXN"
      },
      "source": [
        "### 2.3 K-Means Clustering with Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esCC3XgJZUj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is all a lot easier when we use a library instead of doing it by hand.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VmyHklDKSI_m",
        "colab": {}
      },
      "source": [
        "# Instantiate the sklearn class, and pick a number of clusters.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzwOb5yLZUkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the instantiate model to our data.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-LeRetwZUkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the labels are the same as the 3 centers.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Gj9sXFDWtLo",
        "colab": {}
      },
      "source": [
        "# Add our new labels to the dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "on8qYidhXaA3",
        "colab": {}
      },
      "source": [
        "# Use our previous function to display the clusters as defined by scikit-learn.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WF5d6AP_VT4c"
      },
      "source": [
        "### 2.6 Important Considerations:\n",
        "\n",
        "- Choosing the appropriate clustering method \n",
        "\n",
        "We've only taught you one so stick with that for today. \n",
        "\n",
        "- Choosing appropriate dimensions to cluster along. \n",
        "\n",
        "Hmmm, what would be the best dimension to cluster along? Maybe one that helps separate the clusters the best. You can do a lot of scatterplots to examine this or you could, I dunno, use a technique that maximizes the variance along certain dimensions transforming the data into principal components and then cluster along the dimensions of the principal components. \n",
        "\n",
        "- Choosing a distance measure\n",
        "\n",
        "Euclidean is the most traditional, you'll learn the others if the occasion presents itself (it most likely won't) - If I'm being completely honest.\n",
        "\n",
        "- Choosing an appropriate k (# of clusters)\n",
        "\n",
        "THIS IS THE MOST IMPORTANT CONSIDERATION WHEN IT COMES TO K-MEANS (I mean it's in the name)\n",
        "\n",
        "![Elbow Method](https://media.geeksforgeeks.org/wp-content/uploads/20190606105550/distortion1.png)\n",
        "\n",
        "On the x-axis we have number of centroids (k)\n",
        "\n",
        "On the y-axis we have \"distortion\" which is measured as the sum of squared distances of each point to its given cluster\n",
        "\n",
        "Here's some code below that could be used to create a similar \"Elbow\" Graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXflkYwTL8WT",
        "colab": {}
      },
      "source": [
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(points)\n",
        "    sum_of_squared_distances.append(km.inertia_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxrwEX4VL-u5",
        "colab": {}
      },
      "source": [
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx2_Mz07ZUkP",
        "colab_type": "text"
      },
      "source": [
        "Silhouette Coefficient -- measure of how far apart clusters are\n",
        "\n",
        "high Silhouette Score = clusters are well separated\n",
        "ranges from -1 to 1\n",
        "The definition is a little involved, but intuitively the score is based on how much closer data points are to their own clusters than to the nearest neighbor cluster.\n",
        "\n",
        "We can calculate it in sklearn with metrics.silhouette_score(X_scaled, labels, metric='euclidean').\n",
        "\n",
        "https://en.wikipedia.org/wiki/Silhouette_(clustering)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KaMm94zZUkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the metrics module will be your best friend\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ivg3yx3ZUkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# what's our silhouette score?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DhMkzbIFX96q"
      },
      "source": [
        "### 2.4 Further Considerations\n",
        "\n",
        "- Choosing an appropriate K\n",
        "\n",
        "- Unlucky Initial Centroids\n",
        "\n",
        "Unlucky Initial Centroids can \n",
        "\n",
        "    - result in a poor clustering\n",
        "    - lead to a clustering that doesn't converge\n",
        "\n",
        "- Computational Complexity\n",
        "\n",
        "- What is K-means good for?\n",
        "\n",
        "- ### Mostly Round, linearly-separable blobs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHrH46mZZUkU",
        "colab_type": "text"
      },
      "source": [
        "## Part 3. Apply K-means clustering to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qiwqv9fJZUkV",
        "colab_type": "text"
      },
      "source": [
        "Isotopic Composition of Plutonium Batches  \n",
        "The pluton data frame has 45 rows and 4 columns, containing percentages of isotopic composition of 45 Plutonium batches.  \n",
        "https://vincentarelbundock.github.io/Rdatasets/doc/cluster/pluton.html  \n",
        "- Pu238: the percentages of (238)Pu, always less than 2 percent.\n",
        "- Pu239: the percentages of (239)Pu, typically between 60 and 80 percent.\n",
        "- Pu240: percentage of the plutonium 240 isotope.\n",
        "- Pu241: percentage of the plutonium 241 isotope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDtQPcp4ZUkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/pluton.csv\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFP-BdjjZUkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use Pu239 and Pu240 as our features.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnd5YHA8ZUka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot them to see how they look.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7dyOz0_ZUkd",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Non-standardized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXXdQJDnZUke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize an instance of the KMeans class from sklearn.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQsQS0o1ZUkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the instantiated model to our sliced dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzLaBtMxZUkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign clusters back to our dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VARc6c3ZUkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get our centroids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj-jMSF8ZUki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn them into a dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWF4Y9pdZUkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup some colors for plotting\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbG1iJ-VZUkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the scatter of our points with calculated centroids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSEbyaMkZUkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# what's our silhouette score?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdlvh71LZUkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What is the predicted centroid for each data point?  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqRyJwslZUkp",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 K-means with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu5aKhktZUkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read back in the data, so it's fresh\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4qJckSkZUkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this time keep all 4 variables\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6ZVSNmVZUks",
        "colab_type": "text"
      },
      "source": [
        "**scale the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-RP5jEdNZUks",
        "colab": {}
      },
      "source": [
        "# instantiate the SKLearn class for standardization\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf6nH8y1ZUkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the dataset (default is normalization)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2BrzhBrZUkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now it's a numpy array, not a dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek_NeaTGZUky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn it back into a dataframe.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gLic0F0ZUk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# So did that work? the mean should be zero\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAZfBamTZUk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and the std should be one.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq8HdgkWZUk4",
        "colab_type": "text"
      },
      "source": [
        "**Now that we've scaled we can apply PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RQDIjwmZUk4",
        "colab": {}
      },
      "source": [
        "# import and instantiate the PCA class\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9PTpgKJZUk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply PCA to the data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1yNuTULZUk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  how much variation did each principal component explain?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-nyBH16ZUk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How much total variance did we explain?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qArq4DU4ZUk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How much information did we lose?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "F9Lk5otfZUk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn that into a dataframe.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShbNGu8QZUk_",
        "colab_type": "text"
      },
      "source": [
        "**now apply clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXwNwhrQZUk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize an instance of the KMeans class from sklearn.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqf46EIWZUlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the instantiated model to our sliced dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_neRyayZUlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign predicted clusters back to our dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezMM6KLmZUlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get our centroids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxqLfv7qZUlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn them into a dataframe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDxYc-VKZUlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup some colors for plotting\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkkkoV5YZUlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the scatter of our points with calculated centroids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3kmdnIyZUlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# what's our silhouette score?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdw6bDiKZUlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What is the predicted centroid for each data point?  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5eJtbfZUlK",
        "colab_type": "text"
      },
      "source": [
        "**K-Means tradeoffs**\n",
        "- Unsupervised clustering model\n",
        "- Iteratively finds labels given K\n",
        "- Easy to implement in sklearn\n",
        "- Sensitive to shape, scale of data\n",
        "- Optimal K hard to evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eux2FHSWZUlK",
        "colab_type": "text"
      },
      "source": [
        "| strengths | weaknesses |\n",
        "|---|---|\n",
        "| K-Means is popular because it's simple and computationally efficient. | However, K-Means is highly scale dependent and isn't suitable for data of varying shapes and densities. |\n",
        "| Easy to see results / intuitive. | Evaluating results is more subjective, requiring much more human evaluation than trusted metrics. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrI_DjsZUlL",
        "colab_type": "text"
      },
      "source": [
        "**Additional Resources**  \n",
        "- Andrew Moore's CS class at Carnegie Mellon contains good static visualization, step-by-step. His slide deck is online here: http://www.cs.cmu.edu/~cga/ai-course/kmeans.pdf. He also links to more of his tutorials on the first page.\n",
        "\n",
        "Some helpful stackexchange questions:\n",
        "\n",
        "- http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust\n",
        "- http://stats.stackexchange.com/questions/174556/k-means-clustering-with-dummy-variables\n",
        "- http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKeiDlkMZUlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}